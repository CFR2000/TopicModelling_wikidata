## UE 803: Data Science
# Project: Clustering and Classifying Articles based on Text and knowledge-base information

## About this project
In this project, we need to collect information about articles belonging to different categories (such as airports, artists, politicians, sportspeople, etc). Based on this information, we will try to automatically cluster and classify these articles into the correct categories. The source of information we are using are:
* Wikipedia Online Encyclopedia
* Wikidata knowledge base

## Installation
Note: Following code has been implemented in Python3

1. cd to the directory where ```requirements.txt``` is located;

2. run: `pip install -r requirements.txt` .

## General Overview
The project constitues of four steps, namely:
1. Corpus Extraction
2. Pre-Processing
3. Clustering
4. Classifying


### Corpus Extraction 
This step extracts information about articles from wikipedia and wikidata. These articles belong to the categories: 
* Airports
* Artists
* Astronauts
* Building
* Astronomical_objects
* City
* Comics_characters
* Companies
* Foods
* Transport
* Monuments_and_memorials
* Politicians
* Sports_teams
* Sportspeople
* Universities_and_colleges
* Written_communication'.

The script used by the Corpus Extraction gets k (**```num_articles```** param) articles from each category and stores the following features such as: description, page content (where the number n of sentences is represented by the **```num_sentences```** param) , infobox, wikidata statements (triples). The result is stored in the 'articles.csv'.

### Pre-Processing
This step takes, as an input, the csv file (articles.csv) generated by the previous step. The data is processed using the following steps:
* Tokenize the text
* Lowercase the tokens
* Remove punctuation and function words
* Remove stop words

The preprocessed output is stored in **```data/preprocessed_data.csv```**

### Clustering
This step takes the **```data/preprocessed_data.csv```** as input which is going to be used to train the KMeans algorithm (--input param). The methods used in order to process the data are:
* Tf-idk
* Token presence
* Token frequency

The results obtained are stored in **```data/Clustering results.csv```** (which shows a comparison table) and **```data/Clustering visualization.png```** (which shows a comparison plot).

### Classifying
This step take the **```data/preprocessed_data.csv```** as input which is going to be used to train the Perceptron algorithm. The method used to preprocess the data is Tf-Idf Vectorizer. The results obtained are stored in the data folder. The file obtatined are:
* Confusion matrix

## Results

The results are obtained on the data stored in the ```data``` directory:

**Clustering**:

![Clustering visualization](https://github.com/schopra6/TopicModelling_wikidata/blob/main/data/Clustering%20visualization.png)

